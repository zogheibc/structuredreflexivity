{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4544e58a",
   "metadata": {},
   "source": [
    "# Quantitative Text Analysis\n",
    "\n",
    "After importing packages and text data, and defining pre-processing, this code does the following:\n",
    "\n",
    "1. Analyze all answers from all datasheets together\n",
    "2. Analyze answers from each individual datasheet\n",
    "3. Analyze answers on a question-by-question basis\n",
    "\n",
    "For each of these categories, we explore:\n",
    "- Term frequency (which words occur most often)\n",
    "- Collocations (which pairs and triplets of words co-occur most often)\n",
    "- Lexical diversity (ratio of unique words to total words)\n",
    "\n",
    "We also conduct topic modeling and calculate TF-IDF cosine similarity to compare across individual datasheets (section 2) and across questions (section 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e366ad",
   "metadata": {},
   "source": [
    "## Load packages and define text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e99a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.matutils import cossim\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "!pip install pyLDAvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe0cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##load nltk datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd3455",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load csv of text data\n",
    "datasheetsContent = pd.read_excel(\"datasheetscontent.xlsx\")\n",
    "####print(datasheetsContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eddb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "##text preprocessing\n",
    "\n",
    "##load resources\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "na_words = [\"nan\", \"NA\", \"N/A\", \"NAN\", \"http\", \"HTTP\", \".com\", \"c\"]\n",
    "na_words = set(na_words)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "##define processing function\n",
    "def process(text):\n",
    "    ##make text lowercase\n",
    "    text = text.lower()\n",
    "    ##tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    ##keep alphabetic tokens only\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    ##remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    ##remove na words\n",
    "    tokens = [word for word in tokens if word not in na_words]\n",
    "    ##filter out urls\n",
    "    tokens = [token for token in tokens if not token.startswith('http://') and not token.startswith('https://')]\n",
    "    ##lemmatize text\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec78c3",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd04471",
   "metadata": {},
   "source": [
    "# 1. Analyze all answers from all datasheets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##concatenate all answers from all datasheets into a single string\n",
    "allAnswersString = \" \".join(datasheetsContent['answer'].astype(str))\n",
    "####print(allAnswersString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71aa2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##process text data\n",
    "\n",
    "lemmatizedWords = process(allAnswersString)\n",
    "####print(lemmatizedWords)\n",
    "\n",
    "####allAnswersString = \" \".join(lemmatizedWords)\n",
    "####print(allAnswersString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c09083",
   "metadata": {},
   "source": [
    "### *Word Frequency*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73125067",
   "metadata": {},
   "outputs": [],
   "source": [
    "##analyze word frequency\n",
    "\n",
    "frequencyDistribution = FreqDist(lemmatizedWords)\n",
    "##print 10 most common words\n",
    "print(frequencyDistribution.most_common(20))\n",
    "##visualize word frequencies\n",
    "fig, ax = plt.subplots(figsize=(12,9), layout=\"constrained\")\n",
    "plt.title(\"Term Frequency\", fontsize=28, pad = 30)\n",
    "frequencyDistribution.plot(20, cumulative=False)  \n",
    "plt.xlabel(\"Term\", fontsize = 18)\n",
    "plt.ylabel(\"Number of Occurrences\", fontsize = 18)\n",
    "for line in ax.get_lines():\n",
    "    line.set_linewidth(5)\n",
    "plt.setp(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    ha=\"right\",\n",
    "    rotation_mode=\"anchor\"\n",
    ")\n",
    "ax.tick_params(axis='x', labelsize = 14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "\n",
    "plt.savefig('frequencyImage.png', dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b5a17",
   "metadata": {},
   "source": [
    "### *Collocations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c8c35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##find collocations\n",
    "\n",
    "##bigrams (pairs of words that occur together often)\n",
    "bigram_finder = BigramCollocationFinder.from_words(lemmatizedWords)\n",
    "###only show those that occur more than thrice\n",
    "bigram_finder.apply_freq_filter(3)\n",
    "###rank by PMI (Pointwise Mutual Information)\n",
    "bigram_collocations = bigram_finder.nbest(BigramAssocMeasures.pmi, 10)\n",
    "\n",
    "print(\"Top bigram collocations with frequency:\")\n",
    "for bigram in bigram_collocations:\n",
    "    freq = bigram_finder.ngram_fd[bigram]\n",
    "    print(f\"{bigram}: {freq}\")\n",
    "    \n",
    "##trigrams (triplets of words that occur together often)\n",
    "trigram_finder = TrigramCollocationFinder.from_words(lemmatizedWords)\n",
    "###only show those that occur more than thrice\n",
    "trigram_finder.apply_freq_filter(3)\n",
    "###rank by PMI\n",
    "trigram_collocations = trigram_finder.nbest(TrigramAssocMeasures.pmi, 10)\n",
    "\n",
    "print(\"\\nTop trigram collocations with frequency:\")\n",
    "for trigram in trigram_collocations:\n",
    "    freq = trigram_finder.ngram_fd[trigram]\n",
    "    print(f\"{trigram}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56aa55",
   "metadata": {},
   "source": [
    "### *Lexical Diversity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035eaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate lexical diversity of all answers across papers/questions\n",
    "totalWords = len(lemmatizedWords)\n",
    "uniqueWords = len(set(lemmatizedWords))\n",
    "lexicalDiversity = uniqueWords / totalWords\n",
    "print(f\"Lexical diversity: {lexicalDiversity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a1a0e",
   "metadata": {},
   "source": [
    "### *Keyword search*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4307365",
   "metadata": {},
   "outputs": [],
   "source": [
    "##primary keyword searching\n",
    "import re\n",
    "\n",
    "keywordList = [\"reflex\", \"reflection\", \"marginalization\", \"power\",\n",
    "              \"hierarchy\", \"oppression\", \"inequity\", \"historical\",\n",
    "              \"social\", \"cultural\", \"socio-cultural\", \"domain\",\n",
    "              \"discipline\", \"institution\", \"norms\", \"neutrality\",\n",
    "              \"objectivity\", \"subjectivity\", \"bias\", \"positionality\",\n",
    "              \"identity\", \"relationality\",\"disclosure\",\"accountability\",\n",
    "              \"transparency\"]\n",
    "\n",
    "for word in keywordList: \n",
    "    if word.lower() in allAnswersString.lower():\n",
    "        print(f\"{word} Found\")\n",
    "        count = len(re.findall(rf\"{word}\", allAnswersString, re.I))\n",
    "        print(count)\n",
    "    else:\n",
    "        print(f\"{word} Not Found\")\n",
    "        count = len(re.findall(rf\"{word}\", allAnswersString, re.I))\n",
    "        print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f55423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##explore keywords in context\n",
    "\n",
    "def keywordsInContext(text, keyword, windowSize=7): ##windowSize defines how many surrounding words to print\n",
    "    words = text.split()\n",
    "    results = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word.lower() == keyword.lower():\n",
    "            start = max(0, i - windowSize)\n",
    "            end = min(len(words), i + windowSize + 1)\n",
    "            context = words[start:end]\n",
    "            results.append(\" \".join(context))\n",
    "    return results\n",
    "\n",
    "##search specific keywords as needed for in-depth exploration\n",
    "keywordsInContext(allAnswersString, \"powerful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd4793",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a906b",
   "metadata": {},
   "source": [
    "# 2. Analyze answers from each individual datasheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4296238",
   "metadata": {},
   "outputs": [],
   "source": [
    "##group data by individual papers\n",
    "paperAnswersString = (\n",
    "    datasheetsContent.groupby(\"paperacronym\")[\"answer\"]\n",
    "      .apply(lambda x: \" \".join(x.astype(str)))\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "###print(paperAnswersString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cce941",
   "metadata": {},
   "outputs": [],
   "source": [
    "##process text data\n",
    "\n",
    "paperAnswersString[\"processedTokens\"] = paperAnswersString[\"answer\"].apply(process)\n",
    "####print(paperAnswersString)\n",
    "\n",
    "groupedTokens = paperAnswersString.groupby('paperacronym')['processedTokens'].sum().reset_index()\n",
    "###print(groupedTokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc3fce",
   "metadata": {},
   "source": [
    "### *Word Frequency*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdadfe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##analyze word frequency for each paper\n",
    "\n",
    "##define function to plot frequency distribution of top 10 words\n",
    "def plotWordFrequency(tokens, title, num_words=20):\n",
    "    frequencyDistribution = FreqDist(tokens)\n",
    "    print(frequencyDistribution.most_common(20))\n",
    "    plt.figure(figsize=(12,6))\n",
    "    frequencyDistribution.plot(num_words, cumulative=False)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "##generate frequency plot for each paper\n",
    "for _, row in groupedTokens.iterrows():\n",
    "    paper = row['paperacronym']\n",
    "    tokens = row['processedTokens']\n",
    "    plotWordFrequency(tokens, f'Word Frequency for {paper}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6b84a",
   "metadata": {},
   "source": [
    "### *Collocations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1652e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##get collocations for each paper\n",
    "\n",
    "##define function to get bigrams\n",
    "def get_bigram(tokens, top_n):\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(2) ##with minimum frequency of 2\n",
    "    collocations = finder.nbest(BigramAssocMeasures.pmi, top_n)\n",
    "    return [(c, finder.ngram_fd[c]) for c in collocations]\n",
    "\n",
    "##define function to get trigrams\n",
    "def get_trigram(tokens, top_n):\n",
    "    finder = TrigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(2)\n",
    "    collocations = finder.nbest(TrigramAssocMeasures.pmi, top_n)\n",
    "    return [(c, finder.ngram_fd[c]) for c in collocations]\n",
    "\n",
    "\n",
    "##for loop; apply the collocations functions to each paper; show top 10 results (top_n)\n",
    "for _, row in groupedTokens.iterrows():\n",
    "    paper = row['paperacronym']\n",
    "    tokens = row['processedTokens']\n",
    "    \n",
    "    print(f\"\\n Paper: {paper} \")\n",
    "    \n",
    "    bigrams = get_bigram(tokens, top_n=10)\n",
    "    print(\"Top bigrams:\")\n",
    "    for collocation, freq in bigrams:\n",
    "        print(f\"{collocation}: {freq}\")\n",
    "    \n",
    "    trigrams = get_trigram(tokens, top_n=10)\n",
    "    print(\"Top trigrams:\")\n",
    "    for collocation, freq in trigrams:\n",
    "        print(f\"{collocation}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020a440",
   "metadata": {},
   "source": [
    "### *Lexical Diversity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5534bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##calculate lexical diversity of answers for each paper\n",
    "\n",
    "##define function to calculate lexical diversity\n",
    "def lexicalDiversity(tokens):\n",
    "    totalWords = len(tokens)\n",
    "    uniqueWords = len(set(tokens))\n",
    "    return uniqueWords / totalWords if totalWords > 0 else 0\n",
    "\n",
    "##apply to each paper\n",
    "groupedTokens['lexicalDiversity'] = groupedTokens['processedTokens'].apply(lexicalDiversity)\n",
    "\n",
    "print(groupedTokens[['paperacronym', 'lexicalDiversity']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6baa3ae",
   "metadata": {},
   "source": [
    "### *TF-IDF and Cosine Similarity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9675d8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##calculate TF-IDF (term frequency-inverse document frequency) to view word importance in each paper, then use cosine similarity to measure how alike the papers are\n",
    "\n",
    "##make it into a single string\n",
    "groupedTokens['text'] = groupedTokens['processedTokens'].apply(lambda x: ' '.join(x))\n",
    "##generate tf-idf vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidfMatrix = vectorizer.fit_transform(groupedTokens['text'])\n",
    "##calculate cosine similarity and display as a dataframe\n",
    "cosineSimilarity = cosine_similarity(tfidfMatrix)\n",
    "cosineSimilarityDataframe = pd.DataFrame(cosineSimilarity, index=groupedTokens['paperacronym'], columns=groupedTokens['paperacronym'])\n",
    "#print(cosineSimilarityDataframe)\n",
    "\n",
    "##style dataframe printout for easier interpretation\n",
    "def bold_if_above_threshold(value, threshold):\n",
    "    if value > threshold:\n",
    "        return 'font-weight: bold'\n",
    "    else:\n",
    "        return ''   \n",
    "styledCosineSimilarityDataframe = cosineSimilarityDataframe.style.applymap(lambda x: bold_if_above_threshold(x, 0.2))\n",
    "print(\"Cosine Similarity Across Papers with values > 0.2 bolded:\" )\n",
    "styledCosineSimilarityDataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db6d61f",
   "metadata": {},
   "source": [
    "### *Topic Modeling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1dc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create dictionary for LDA\n",
    "texts = groupedTokens['processedTokens'].tolist() ##makes each paper's text content into a list of words\n",
    "dictionary = corpora.Dictionary(texts) ##maps unique IDs to each unique word in the dataset\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] ##bag of words format; every document (in this case, papers) is represented as a list of each word ID and count of its occurences in that document\n",
    "\n",
    "\n",
    "##train LDA model\n",
    "numTopics = 10  ##choose a number of topics to elicit\n",
    "lda = models.LdaModel(corpus, num_topics=numTopics, id2word=dictionary, random_state=42)\n",
    "\n",
    "##evaluate LDA model\n",
    "##first calculate perplexity (lower perplexity = how well our number of topics captures the actual distribution of words in the document = better)\n",
    "perplexity_score = lda.log_perplexity(corpus)\n",
    "print(f'Perplexity: {perplexity_score}')\n",
    "##calculate coherence (higher coherence = higher word similarity across the topic = better)\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda,\n",
    "    texts=texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence: {coherence_score}')\n",
    "\n",
    "\n",
    "##view topics (global topics across all papers)\n",
    "print(\"\\nLDA Topics:\")\n",
    "for idx, topic in lda.print_topics(-1):\n",
    "    print(f\"Topic {idx+1}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115f317",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##fancy interactive topic modeling viz\n",
    "####THROWS A LOT OF WARNINGS BECAUSE THE PACKAGE SOURCE CODE USES AN OLD DATETIME VERSION\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc18206",
   "metadata": {},
   "outputs": [],
   "source": [
    "##export saliency bar graph to png (recreating in matplotlib)\n",
    "\n",
    "##get data from pyldavis/gensim\n",
    "topicInfo = vis.topic_info.copy()\n",
    "salientTerms = topicInfo[topicInfo[\"Category\"] == \"Default\"]\n",
    "##calculate saliency\n",
    "salientTerms[\"saliency\"] = salientTerms[\"Total\"] * salientTerms[\"loglift\"]\n",
    "##organize salience values from highest to lowest (***this will make our graph look a little different from the PyLDAvis output because that bar plot sorts most salient terms by frequency)\n",
    "salientTerms = salientTerms.sort_values(\n",
    "    by=\"saliency\", ascending=False\n",
    ").head(30)\n",
    "\n",
    "##make plot\n",
    "fig, ax = plt.subplots(figsize=(12, 9), layout=\"constrained\")\n",
    "\n",
    "ax.barh(\n",
    "    salientTerms[\"Term\"],\n",
    "    salientTerms[\"saliency\"]\n",
    ")\n",
    "ax.invert_yaxis()\n",
    "ax.set_title(\"Most Salient Terms\", fontsize=28, pad=30)\n",
    "ax.set_xlabel(\"Saliency\", fontsize=18)\n",
    "ax.set_ylabel(\"Term\", fontsize=18)\n",
    "ax.tick_params(axis=\"x\", labelsize=14)\n",
    "ax.tick_params(axis=\"y\", labelsize=14)\n",
    "\n",
    "##save image as png\n",
    "fig.savefig(\"saliency.png\", dpi=300)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot proportion of topics in each paper\n",
    "\n",
    "paperTopics = [lda.get_document_topics(bow, minimum_probability=0) for bow in corpus]\n",
    "\n",
    "##make dataframe of the probability of each topic occuring in each document\n",
    "topicDistributions = pd.DataFrame(\n",
    "    [[topic_prob for topic_id, topic_prob in doc] for doc in paperTopics],\n",
    "    columns=[f'Topic {i+1}' for i in range(numTopics)]\n",
    ")\n",
    "\n",
    "##identifying each document by paperacronym\n",
    "topicDistributions['paperacronym'] = groupedTokens['paperacronym'].values\n",
    "##set paperacronym as index\n",
    "topicDistributions.set_index('paperacronym', inplace=True)\n",
    "\n",
    "##make stacked barplot\n",
    "topicDistributions.plot(kind='bar', stacked=True, figsize=(12,6), colormap='tab10')\n",
    "plt.ylabel(\"Proportion of Topic\")\n",
    "plt.xlabel(\"Paper\")\n",
    "plt.title(\"Topics by Paper\")\n",
    "plt.legend(title=\"Topics\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeedd22",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5197c6",
   "metadata": {},
   "source": [
    "# 3. Analyze answers on a question-by-question basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36bff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "##group data by questions\n",
    "questionAnswersString = (\n",
    "    datasheetsContent.groupby(\"questionum\")[\"answer\"]\n",
    "      .apply(lambda x: \" \".join(x.astype(str)))\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "###print(questionAnswersString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8dbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "##process text data\n",
    "\n",
    "questionAnswersString[\"processedTokens\"] = questionAnswersString[\"answer\"].apply(process)\n",
    "###print(groupedAnswersString)\n",
    "\n",
    "groupedQuestionTokens = questionAnswersString.groupby('questionum')['processedTokens'].sum().reset_index()\n",
    "###print(groupedQuestionTokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d69582f",
   "metadata": {},
   "source": [
    "### *Word Frequency*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf66af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##analyze word frequency for each question\n",
    "\n",
    "##define function to plot frequency distribution of top 10 words\n",
    "def plotWordFrequency(tokens, title, num_words=20):\n",
    "    frequencyDistribution = FreqDist(tokens)\n",
    "    print(frequencyDistribution.most_common(20))\n",
    "    plt.figure(figsize=(12,6))\n",
    "    frequencyDistribution.plot(num_words, cumulative=False)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "##generate frequency plot for each question\n",
    "for _, row in groupedQuestionTokens.iterrows():\n",
    "    question = row['questionum']\n",
    "    tokens = row['processedTokens']\n",
    "    plotWordFrequency(tokens, f'Word Frequency for Question {question} Answers')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a3ccc0",
   "metadata": {},
   "source": [
    "### *Collocations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe00317",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##get collocations for each question\n",
    "\n",
    "##define function to get bigrams\n",
    "def get_bigrams(tokens, top_n=10):\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(2)\n",
    "    bigrams = finder.nbest(BigramAssocMeasures.pmi, top_n)\n",
    "    return [(b, finder.ngram_fd[b]) for b in bigrams]\n",
    "\n",
    "##define function to get trigrams\n",
    "def get_trigrams(tokens, top_n=10):\n",
    "    finder = TrigramCollocationFinder.from_words(tokens)\n",
    "    finder.apply_freq_filter(2)\n",
    "    trigrams = finder.nbest(TrigramAssocMeasures.pmi, top_n)\n",
    "    return [(t, finder.ngram_fd[t]) for t in trigrams]\n",
    "\n",
    "\n",
    "for _, row in groupedQuestionTokens.iterrows():\n",
    "    question = row['questionum']\n",
    "    tokens = row['processedTokens']\n",
    "    \n",
    "    print(f\"\\n Question {question}:\")\n",
    "    \n",
    "    bigrams = get_bigrams(tokens, top_n=10)\n",
    "    print(\"Top bigrams:\")\n",
    "    for collocation, freq in bigrams:\n",
    "        print(f\"{collocation}: {freq}\")\n",
    "    \n",
    "    trigrams = get_trigrams(tokens, top_n=10)\n",
    "    print(\"Top trigrams:\")\n",
    "    for collocation, freq in trigrams:\n",
    "        print(f\"{collocation}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a3ed4",
   "metadata": {},
   "source": [
    "### *Lexical Diversity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc2954",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##calculate lexical diversity of answers for each question\n",
    "\n",
    "##define function to calculate lexical diversity\n",
    "def lexicalDiversity(tokens):\n",
    "    totalWords = len(tokens)\n",
    "    uniqueWords = len(set(tokens))\n",
    "    return uniqueWords / totalWords if totalWords > 0 else 0\n",
    "\n",
    "##apply to each paper\n",
    "groupedQuestionTokens['lexicalDiversity'] = groupedQuestionTokens['processedTokens'].apply(lexicalDiversity)\n",
    "\n",
    "print(groupedQuestionTokens[['questionum', 'lexicalDiversity']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae9aa6",
   "metadata": {},
   "source": [
    "### *TF-IDF and Cosine Similarity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23811d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##calculate TF-IDF (term frequency-inverse document frequency) to view word importance for each question (ACROSS ALL PAPERS), then use cosine similarity to measure how alike the questions are\n",
    "\n",
    "##make it into a single string\n",
    "groupedQuestionTokens['text'] = groupedQuestionTokens['processedTokens'].apply(lambda x: ' '.join(x))\n",
    "##generate tf-idf vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidfMatrix = vectorizer.fit_transform(groupedQuestionTokens['text'])\n",
    "##calculate cosine similarity and display as a dataframe\n",
    "cosineSimilarityQuestions = cosine_similarity(tfidfMatrix)\n",
    "cosineSimilarityQuestionsDataframe = pd.DataFrame(cosineSimilarityQuestions, index=groupedQuestionTokens['questionum'], columns=groupedQuestionTokens['questionum'])\n",
    "##style dataframe printout for easier interpretation\n",
    "def bold_if_above_threshold(value, threshold):\n",
    "    if value > threshold:\n",
    "        return 'font-weight: bold'\n",
    "    else:\n",
    "        return ''   \n",
    "styledCosineSimilarityQuestionsDataframe = cosineSimilarityQuestionsDataframe.style.applymap(lambda x: bold_if_above_threshold(x, 0.3))\n",
    "print(\"Cosine Similarity Across Questions with values > 0.3 bolded:\" )\n",
    "styledCosineSimilarityQuestionsDataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caabf528",
   "metadata": {},
   "source": [
    "### *Topic Modeling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create dictionary for LDA ACROSS EACH QUESTION THIS TIME\n",
    "texts = groupedQuestionTokens['processedTokens'].tolist() ##makes each question's total answer text content into a list of words\n",
    "dictionary = corpora.Dictionary(texts) ##maps unique IDs to each unique word in the dataset\n",
    "corpus = [dictionary.doc2bow(text) for text in texts] ##bag of words format; every document (in this case, questions) is represented as a list of each word ID and count of its occurences in that document\n",
    "\n",
    "\n",
    "##train LDA model\n",
    "numTopics = 7  ##choose a number of topics to elicit\n",
    "lda = models.LdaModel(corpus, num_topics=numTopics, id2word=dictionary, random_state=42)\n",
    "\n",
    "##evaluate LDA model\n",
    "##first calculate perplexity (lower perplexity = how well our number of topics captures the actual distribution of words in the document = better)\n",
    "perplexity_score = lda.log_perplexity(corpus)\n",
    "print(f'Perplexity: {perplexity_score}')\n",
    "##calculate coherence (higher coherence = higher word similarity across the topic = better)\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda,\n",
    "    texts=texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence: {coherence_score}')\n",
    "\n",
    "##view topics (global topics across all questions)\n",
    "print(\"\\nLDA Topics:\")\n",
    "for idx, topic in lda.print_topics(-1):\n",
    "    print(f\"Topic {idx+1}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc466729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "##fancy interactive topic modeling viz\n",
    "####THROWS A LOT OF WARNINGS BECAUSE THE PACKAGE SOURCE CODE USES AN OLD DATETIME VERSION\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e150074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot proportion of topics across each questions\n",
    "\n",
    "questionTopics = [lda.get_document_topics(bow, minimum_probability=0) for bow in corpus]\n",
    "\n",
    "##make dataframe of the probability of each topic occuring in each document\n",
    "questionTopicDistributions = pd.DataFrame(\n",
    "    [[topic_prob for topic_id, topic_prob in doc] for doc in questionTopics],\n",
    "    columns=[f'Topic {i+1}' for i in range(numTopics)]\n",
    ")\n",
    "\n",
    "##identifying each document by questionnum\n",
    "questionTopicDistributions['questionum'] = groupedQuestionTokens['questionum'].values\n",
    "##set questionnum as index\n",
    "questionTopicDistributions.set_index('questionum', inplace=True)\n",
    "\n",
    "##make stacked barplot\n",
    "questionTopicDistributions.plot(kind='bar', stacked=True, figsize=(12,6), colormap='tab10')\n",
    "plt.ylabel(\"Proportion of Topic\")\n",
    "plt.xlabel(\"Question\")\n",
    "plt.title(\"Topics by Question\")\n",
    "plt.legend(title=\"Topics\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
